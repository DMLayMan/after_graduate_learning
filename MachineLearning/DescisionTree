descision tree   -- 决策树
叶子节点 --  存储着训练数据集中的数据
非叶子节点 --  存储着分类、决策的特征向量以及对应的特征值

----------------------------------------------------------
引入熵的概念：描述一个系统的混乱程度。
H(x) = -Σpi * log pi

提升增益 g(D, A) = H(D) - H(D|A) : 量化在对数据集合D使用特征A进行分类后得到的被划分的数据集合{D1...Dk}，前后两个状态下的熵值的变化量。
也用此【g(D, A)】来作为衡量特征A对集合D划分的成效。
选取 max g(D, A) 的特征作为决策变量。

algorithm ID3：
  每次递归选择最佳的决策特征A作为划分依据，直至满足一定的结束要求，推出递归，结束决策树的构建。---->  返回一棵决策树
  ---满足一定的要求，包括最大的g(D,A)<θ 设定的阈值。
 
algorithm C4.5 和 algorithm ID3 之间唯一的差异就是 C4.5使用的使增益比--g(D,A)/H(D)来选取最佳划分的特征值

叶子节点的数据集使用的同样是多数表决的策咯。
----------------------------------------------------------
CART: classfication and reggression tree

引入基尼的概念:同样也是衡量一个系统的混乱程度。 
Gini(D)= Σpi(1-pi)= 1- Σpi^2

最小二乘回归决策树:【二叉树，是or否，大于or小于等于......】
基本思想：将数据空间划分为N个独立的空间，每个空间对应的有一个预测值。对于待预测的目标，利用该决策树找到其所属的区域，输出对应区域的预测值。
数据集D，在特征 A=a 的决策下被划分为D1、D2两个子空间：
D1 = {(x,y)| xA < a}  
D2 = {(x,y)| xA >=a}
对应空间内最佳的预测值是取空间内的数据点的均值 mean --c1 c2
求得
  min{ min Σ(yi - c1)^2 + min Σ(yi - c2)^2 }
最小的特征以及对应的特征值 -- 这也就是每次的决策、划分依据


--------------还有一个记得不太清楚  remain



























