1.variable 
--------变量、常量 都必须是要声明了
 state = tf.Variable(0, name='counter)
 one = tf.constant(1)
 
 new_value = tf.add(state, one)
 update = tf.assign(state, new_value) 
 
 init = tf.global_veriable_initializer()
 
 with tf.Session() as sess
    for i in range(100):
        sess.run(update)
        print(sess.run(update))
 
 2. placeholder
 
 用 dictionary 键值对的方式进行传值
 placeholder 和 feed_dict 是连着用的。
    
    input1 = tf.placeholder(tf.float32)
    input2 = tf.placeholder(tf.float32)
    
    output = tf.add(input1, input2)
    
    sess = tf.Session()
    print(sess.run(output, feed_dict={input1: 10, input2: 5)
    
    --------------------------50.0-------------------
 
 3.激励函数  activation function  
    其实激励函数就是将当前的值，进行一个非线性或者是线性的变换
      x* = sigmoid(x)     
      relu 
    激励函数的设置一方面能够减弱部分负值的干扰，增强正值的刺激--------说的有点单纯 ， 其实不是正负判别的函数
    
 4.add_layer
    ---------------------------
    a> 每个层都需要有一个输入--inputs 向量、 输入的大小 in_size 、 输出的大小 out_size、以及该层是否有一个激励函数 activation function
    b> 在每层里面都需要执行的 outputs = Weights * inputs + biases  如果没有激励函数就直接输出 否则输出 activation_function(outputs)
    ----------------------------
    
    def add_layer(inputs, in_size, out_size, activation_function=None):
        Weights = tf.Variable(tf.random_normal([in_size, out_size])
        biases = tf.Variable(tf.zeros([1, out_size] + 0.1)
        Wx_plus_b = tf.matmul(Weights, inputs) + biases 
        if activation_function is None:
            outputs = Wx_plus_b
        else:
            outputs = activation_function(Wx_plus_b)
        return outputs;
    
    
    5.构建一个简单地神经网络        1   10    1
     # y = x^2 -0.5    ---用这个来获取输入 （x, y）
      
     x_data = np.linspace(-1, 1, 300)[:,np.newaxis]
     noise = np.random.normal(0, 0.05, x_data.shape)
     y_data = tf.square(x_data) - 0.5 + noise
     
     
     ----------至此就是完成了 输入数据集的构建---------
     xs = tf.placeholder(tf.float32, [None, 1])                --------[None,1]不太懂
     ys = tf.placeholder(tf.float32, [None, 1])
     
     -----------这是对应x_data 和 y_data两个的占位符-----------
    
     l1 = add_layer(xs, 1, 10, tf.nn.relu)                     -------用relu作为激励函数
     prediction = add_layer(l1, 10, 1, activation_function=None)
     
     loss = tf.reduce_mean(tf.reduce_sum(tf.square(ys - prediction), reduction_indices=1))
    
     train_step = tf.train.GradientDescentOptimizer(0.1).minimize(loss)
     
     init = tf.global_variables_initializer()
     
     with tf.Session() as sess:
        sess.run(init)
        for i in range(1000):
          sess.run(train_step, feed_dict={xs: x_data, ys: y_data})
      -----------------------------------------至此就大功告成了    
          if i % 50 == 0:
            print(sess.run(loss, feed_dict={xs: x_data, ys: y_data})
            
            
    6.数据可视化
        pip install matplotlib
        
        import matplotlib.pyplot as plt
        
        
        fig = plt.figure()
        ax = fig.add_subplot(1, 1, 1)
        ax.scatter(x_data, y_data)
        plt.ion()


        for i in range(1000):
          sess.run(train_step, feed_dict={xs: x_data, ys: y_data})
          if i % 50 == 0:
            # print(sess.run(loss, feed_dict={xs: x_data, ys: y_data}))
            try:
              ax.lines.remove(lines[0])
            except Exception:
              pass
            prediction_value = sess.run(prediction, feed_dict={xs: x_data})
            lines = ax.plot(x_data, prediction_value, 'r-', lw=5)
            plt.pause(0.2)
         print("end")
         plt.pause(100)
-----------------------------------------------------下班   哇  没保存 真的要人命
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
